{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function meant to quickly scrape information off of a standard Rotowire page.\n",
    "\n",
    "def scrape_twitter_links(page, xpath):\n",
    "    page.find_element_by_xpath(xpath).click()\n",
    "    soup = BeautifulSoup(page.page_source, 'html.parser')\n",
    "    hyperlinks = [subSoup.find_all('a') for subSoup in soup.find_all('div', {'class':'news-update is-injured'})]\n",
    "    hyperlinks = [links for nested_hyperlinks in hyperlinks for links in nested_hyperlinks]\n",
    "    hyperlinks = [subSoup['href'] for subSoup in hyperlinks if 'twitter' in subSoup['href']]\n",
    "    accounts = set([twitterLink.split('/')[3] for twitterLink in hyperlinks])\n",
    "    accounts = {accountNames + '\\n' for accountNames in accounts}\n",
    "    return accounts, hyperlinks\n",
    "\n",
    "# A helper function meant to be used with a list comprehension for NBC Sportsedge.\n",
    "\n",
    "def helper_function2(subSoup):\n",
    "    return subSoup.find('a', {'class': 'source-title'})['href']\n",
    "\n",
    "# A re-implementation of the ifelse function from R. This is not strictly necessary to write,\n",
    "# but I find it convienient to work with in the Pandas Dataframe.\n",
    "\n",
    "def ifelse(boolean, ifValue, elseValue):\n",
    "    if boolean:\n",
    "        return ifValue\n",
    "    else:\n",
    "        return elseValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accounts and injury tweets from NBC SportsEdge.\n",
    "\n",
    "page = webdriver.Firefox()\n",
    "page.get('https://www.nbcsportsedge.com/edge/baseball/mlb/player-news')\n",
    "time.sleep(10)\n",
    "\n",
    "soup = BeautifulSoup(page.page_source, 'html.parser')\n",
    "page.close()\n",
    "\n",
    "soup.find_all('div', {'class':'player-news-article__source'})\n",
    "links = [helper_function2(subSoup) for subSoup in soup.find_all('div', {'class':'player-news-article__source'})]\n",
    "filtered_links = [filtered_links for filtered_links in links if 'twitter' in filtered_links]\n",
    "accounts = set([links.split('/')[3] for links in filtered_links])\n",
    "accounts = {accountNames + '\\n' for accountNames in accounts}\n",
    "\n",
    "file = open('accountList.txt', 'r')\n",
    "text = file.readlines()\n",
    "file.close()\n",
    "\n",
    "with open('accountList.txt', 'w') as myfile:\n",
    "    for twitterAccounts in set(text).union(accounts):\n",
    "        myfile.write(twitterAccounts)\n",
    "        \n",
    "print(len(set(text).union(accounts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information off of Rotowire.\n",
    "\n",
    "page = webdriver.Firefox()\n",
    "page.get('https://www.rotowire.com/baseball/news.php?injuries=all')\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "accounts, hyperlinks = scrape_twitter_links(page, '/html/body/div[1]/div/main/div[1]/div/div[2]/div/a[6]')\n",
    "time.sleep(2)\n",
    "accounts2, hyperlinks2 = scrape_twitter_links(page, '/html/body/div[1]/div/main/div[1]/div/div[2]/div/a[7]')\n",
    "\n",
    "accounts = accounts.union(accounts2)\n",
    "hyperlinks = hyperlinks + hyperlinks2\n",
    "\n",
    "page.close()\n",
    "\n",
    "file = open('accountList.txt', 'r')\n",
    "text = file.readlines()\n",
    "file.close()\n",
    "\n",
    "with open('accountList.txt', 'w') as myfile:\n",
    "    for twitterAccounts in set(text).union(accounts):\n",
    "        myfile.write(twitterAccounts)\n",
    "        \n",
    "print(len(set(text).union(accounts)))\n",
    "        \n",
    "with open('labeledTweets.txt', 'w') as myfile:\n",
    "    for labeledData in set([links + '\\n' for links in hyperlinks]):\n",
    "        myfile.write(labeledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "250  usernames reached.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500  usernames reached.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "750  usernames reached.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n",
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1000  usernames reached.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1250  usernames reached.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1500  usernames reached.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1750  usernames reached.\n"
     ]
    }
   ],
   "source": [
    "# Use the account information from Rotowire and SportsEdge to collect tweets using Twint.\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "file = open('accountList.txt', 'r')\n",
    "text = file.readlines()\n",
    "file.close()\n",
    "userids = [userid.rstrip('\\n') for userid in text]\n",
    "os.chdir(os.getcwd() + '\\\\TweetData')\n",
    "\n",
    "broken_ids = list()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for usernames in userids:\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    try:\n",
    "\n",
    "        c = twint.Config()\n",
    "        c.Username = usernames\n",
    "        c.Limit = 100\n",
    "        c.Store_csv = True\n",
    "        c.Output = usernames + \".csv\"\n",
    "        c.Hide_output=True\n",
    "\n",
    "        twint.run.Search(c)\n",
    "        time.sleep(15)\n",
    "        \n",
    "    except ValueError:\n",
    "        broken_ids.append(usernames)\n",
    "        \n",
    "    if (count % 250) == 0:\n",
    "        print(count, ' usernames reached.')\n",
    "        \n",
    "os.chdir(os.getcwd()[:-10])\n",
    "        \n",
    "if len(broken_ids) > 0:\n",
    "    \n",
    "    broken_ids = set([id + '\\n' for id in broken_ids])\n",
    "    userids = set([userid + '\\n' for userid in userids])\n",
    "        \n",
    "    with open('toBeFixed.txt', 'w') as myfile:\n",
    "        for labeledData in broken_ids:\n",
    "            myfile.write(labeledData)\n",
    "    \n",
    "    with open('accountList.txt', 'w') as myfile:\n",
    "        for labeledData in userids - broken_ids:\n",
    "            myfile.write(labeledData)\n",
    "    \n",
    "fileList = [os.getcwd()  + '\\\\TweetData\\\\' + files for files in os.listdir(os.getcwd()  + '\\\\TweetData')]\n",
    "\n",
    "data = pd.read_csv('merged.csv')\n",
    "\n",
    "for i in range(len(fileList)):\n",
    "    data = data.append(pd.read_csv(fileList[i]))\n",
    "    \n",
    "data = data.drop_duplicates()\n",
    "data.to_csv('merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the data from merged.csv to send to filtered.\n",
    "\n",
    "raw_data = pd.read_csv('merged.csv')\n",
    "\n",
    "raw_data['urls'] = raw_data['urls'].apply(lambda x: str(x).lstrip(\"[\").rstrip(\"]\"))\n",
    "raw_data['link_present'] = raw_data['urls'].apply(lambda x: ifelse(len(x) > 0, 1, 0))\n",
    "raw_data['photos'] = raw_data['photos'].apply(lambda x: str(x).lstrip(\"[\").rstrip(\"]\"))\n",
    "raw_data['photo_present'] = raw_data['photos'].apply(lambda x: ifelse(len(x) > 0, 1, 0))\n",
    "raw_data['retweet'] = raw_data['retweet'].apply(lambda x: ifelse(x, 1, 0))\n",
    "\n",
    "file = open('labeledTweets.txt', 'r')\n",
    "labeledData = file.readlines()\n",
    "file.close()\n",
    "\n",
    "labeledData = [tweets.rstrip('\\n') for tweets in labeledData]\n",
    "raw_data['injury_report'] = raw_data['link'].apply(lambda x: '1' if x in labeledData else 'x')\n",
    "\n",
    "raw_data = raw_data[['injury_report', 'retweet', 'photo_present', 'link_present', 'replies_count', 'retweets_count',\n",
    "     'likes_count', 'username', 'tweet']]\n",
    "\n",
    "filtered_data = pd.read_csv('filtered.csv')\n",
    "\n",
    "key_diff = set(raw_data.tweet).difference(filtered_data.tweet)\n",
    "where_diff = raw_data.tweet.isin(key_diff)\n",
    "\n",
    "filtered_data = filtered_data.append(raw_data[where_diff], ignore_index=True)\n",
    "\n",
    "filtered_data.to_csv('filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the duplicate data (example: retweets) picked up is already labeled.\n",
    "\n",
    "data = pd.read_csv('filtered.csv')\n",
    "\n",
    "tweets = data[data['injury_report'] != 'x']['tweet'].value_counts(ascending=False)\n",
    "tweets = tweets[tweets > 1].index\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    \n",
    "    if '0' in list(data[data['tweet'] == tweets[i]]['injury_report']):\n",
    "        data.loc[data[data['tweet'] == tweets[i]].index, 'injury_report'] = '0'\n",
    "        \n",
    "    elif '1' in list(data[data['tweet'] == tweets[i]]['injury_report']):\n",
    "        data.loc[data[data['tweet'] == tweets[i]].index, 'injury_report'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operation for quickly labeling a group of duplicated tweets.\n",
    "\n",
    "#data[data['injury_report'] == 'x']['tweet'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If all not injury data, run this to label all as 0 immediately.\n",
    "\n",
    "#noninjuries = data[data['injury_report'] == 'x']['tweet'].value_counts().head(20).index\n",
    "\n",
    "#for i in range(len(noninjuries)):\n",
    "#    data.loc[data[data['tweet'] == noninjuries[i]].index, 'injury_report'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise, quickly label what is injury data.\n",
    "\n",
    "#tweet = data[data['injury_report'] == 'x']['tweet'].value_counts().head(20).index[0]\n",
    "\n",
    "#data.loc[data[data['tweet'] == tweet].index, 'injury_report'] = '1'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
