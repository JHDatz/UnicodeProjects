{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhd15\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_incremental_pca.py:313: RuntimeWarning: Mean of empty slice.\n",
      "  explained_variance[self.n_components_:].mean()\n",
      "C:\\Users\\jhd15\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "# Fit IPCA first to avoid running into memory constraints.\n",
    "# IPCA had to be done over the labeled data only; the full dataset would take 24 hours to compute.\n",
    "# This results in a much less accurate lower dim representation of TF-IDF data, which is done over\n",
    "# the entire dataset.\n",
    "\n",
    "chunksize = 1000\n",
    "data = pd.read_csv('filtered.csv')\n",
    "v = TfidfVectorizer()\n",
    "v.fit(data['tweet'].dropna())\n",
    "\n",
    "labeled_data = data[data['injury_report'] != 'x']\n",
    "\n",
    "IPCA = IncrementalPCA(n_components=1000, batch_size=chunksize, copy=False)\n",
    "count = 0\n",
    "for i in range(0, labeled_data.shape[0] // chunksize):\n",
    "    IPCA.partial_fit(v.transform(labeled_data['tweet'][count*chunksize:(count+1)*chunksize]).toarray())\n",
    "    print((count+1)*chunksize)\n",
    "    count += 1\n",
    "    \n",
    "IPCA.batch_size_ = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly partition the data and convert it to Count, Boolean, and TF-IDF Formats.\n",
    "\n",
    "Xtr, Xte, Ytr, Yte =  model_selection.train_test_split(labeled_data['tweet'],\n",
    "                                                       labeled_data['injury_report'].astype(int),\n",
    "                                                       test_size=0.3)\n",
    "\n",
    "Xtr_tfidf = v.transform(Xtr)\n",
    "Xte_tfidf = v.transform(Xte)\n",
    "Xtr_pca = IPCA.transform(Xtr_tfidf)\n",
    "Xte_pca = IPCA.transform(Xte_tfidf)\n",
    "\n",
    "v = CountVectorizer(binary=True)\n",
    "\n",
    "v.fit(data['tweet'].dropna())\n",
    "Xtr_bool = v.transform(Xtr)\n",
    "Xte_bool = v.transform(Xte)\n",
    "\n",
    "v = CountVectorizer()\n",
    "\n",
    "v.fit(data['tweet'].dropna())\n",
    "Xtr_count = v.transform(Xtr)\n",
    "Xte_count = v.transform(Xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4212    4]\n",
      " [ 127  186]]\n",
      "KNeighborsClassifier(n_neighbors=1, weights='distance')\n"
     ]
    }
   ],
   "source": [
    "# K Nearest Neighbors over TF-IDF representation\n",
    "\n",
    "kNN = KNeighborsClassifier()\n",
    "param_search = [{'weights': ['distance'], 'n_neighbors': [i for i in range(1,51)]}]\n",
    "\n",
    "grid_search = GridSearchCV(kNN, param_search, cv=8, scoring='recall', n_jobs=3)\n",
    "grid_search.fit(Xtr_tfidf, Ytr)\n",
    "\n",
    "y_hat = grid_search.best_estimator_.predict(Xte_tfidf)\n",
    "\n",
    "print(confusion_matrix(Yte, y_hat))\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 10000 candidates, totalling 80000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  43 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=3)]: Done 285 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=3)]: Done 691 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=3)]: Done 1257 tasks      | elapsed:   31.3s\n",
      "[Parallel(n_jobs=3)]: Done 1987 tasks      | elapsed:   47.6s\n",
      "[Parallel(n_jobs=3)]: Done 2877 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=3)]: Done 3931 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=3)]: Done 5145 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=3)]: Done 6523 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=3)]: Done 8061 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=3)]: Done 9763 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=3)]: Done 11625 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=3)]: Done 13651 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=3)]: Done 15837 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=3)]: Done 18187 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=3)]: Done 20697 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=3)]: Done 23371 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=3)]: Done 26205 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=3)]: Done 29203 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=3)]: Done 32361 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=3)]: Done 35683 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=3)]: Done 39165 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=3)]: Done 42811 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=3)]: Done 46617 tasks      | elapsed: 17.5min\n",
      "[Parallel(n_jobs=3)]: Done 50587 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=3)]: Done 54717 tasks      | elapsed: 20.5min\n",
      "[Parallel(n_jobs=3)]: Done 59011 tasks      | elapsed: 22.0min\n",
      "[Parallel(n_jobs=3)]: Done 63465 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=3)]: Done 68083 tasks      | elapsed: 25.3min\n",
      "[Parallel(n_jobs=3)]: Done 72861 tasks      | elapsed: 27.0min\n",
      "[Parallel(n_jobs=3)]: Done 77803 tasks      | elapsed: 28.8min\n",
      "[Parallel(n_jobs=3)]: Done 80000 out of 80000 | elapsed: 29.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4149   67]\n",
      " [  53  260]]\n",
      "BernoulliNB(alpha=0.0013001300130013002)\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli Naive Bayes; must be done over Boolean Representaion\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "\n",
    "param_search = [{'alpha': list(np.linspace(0, 1, 10000))}]\n",
    "\n",
    "grid_search = GridSearchCV(BNB, param_search, cv=8, scoring='recall', n_jobs=3, verbose=2)\n",
    "grid_search.fit(Xtr_bool, Ytr)\n",
    "\n",
    "y_hat = grid_search.best_estimator_.predict(Xte_bool)\n",
    "\n",
    "print(confusion_matrix(Yte, y_hat))\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 10000 candidates, totalling 80000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done 178 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=3)]: Done 1146 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=3)]: Done 2770 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=3)]: Done 5034 tasks      | elapsed:   59.3s\n",
      "[Parallel(n_jobs=3)]: Done 7954 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 11514 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=3)]: Done 15730 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=3)]: Done 20586 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=3)]: Done 26098 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=3)]: Done 32250 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=3)]: Done 39058 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=3)]: Done 46506 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=3)]: Done 54610 tasks      | elapsed: 11.3min\n",
      "[Parallel(n_jobs=3)]: Done 63354 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=3)]: Done 72754 tasks      | elapsed: 15.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4103  113]\n",
      " [  45  268]]\n",
      "MultinomialNB(alpha=0.002000200020002)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 80000 out of 80000 | elapsed: 16.6min finished\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes; must be done over Count Representation\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "\n",
    "param_search = [{'alpha': list(np.linspace(0, 1, 10000))}]\n",
    "\n",
    "grid_search = GridSearchCV(MNB, param_search, cv=8, scoring='recall', n_jobs=3, verbose=2)\n",
    "grid_search.fit(Xtr_count, Ytr)\n",
    "\n",
    "y_hat = grid_search.best_estimator_.predict(Xte_count)\n",
    "\n",
    "print(confusion_matrix(Yte, y_hat))\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 42 candidates, totalling 336 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:   47.6s\n",
      "[Parallel(n_jobs=3)]: Done 336 out of 336 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4098  118]\n",
      " [  36  277]]\n",
      "LogisticRegression(C=1, class_weight='balanced', solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression over TF-IDF Representation\n",
    "\n",
    "LReg = LogisticRegression()\n",
    "\n",
    "param_search = [{'penalty': ['l1', 'l2'],\n",
    "                'C': [2 ** i for i in range(-5, 16)],\n",
    "                'class_weight': ['balanced'],\n",
    "                'solver': ['liblinear']}]\n",
    "\n",
    "grid_search = GridSearchCV(LReg, param_search, cv=8, scoring='recall', n_jobs=3, verbose=2)\n",
    "grid_search.fit(Xtr_tfidf, Ytr)\n",
    "\n",
    "y_hat = grid_search.best_estimator_.predict(Xte_tfidf)\n",
    "\n",
    "print(confusion_matrix(Yte, y_hat))\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 84 candidates, totalling 672 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=3)]: Done 359 tasks      | elapsed: 39.9min\n",
      "[Parallel(n_jobs=3)]: Done 642 tasks      | elapsed: 73.9min\n",
      "[Parallel(n_jobs=3)]: Done 672 out of 672 | elapsed: 77.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4052  164]\n",
      " [  70  243]]\n",
      "RandomForestClassifier(class_weight='balanced_subsample', max_depth=39,\n",
      "                       min_samples_leaf=5, min_samples_split=5,\n",
      "                       n_estimators=1000)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest over Boolean Representation\n",
    "\n",
    "Forest = RandomForestClassifier()\n",
    "\n",
    "\n",
    "param_search = [{'criterion': ['entropy', 'gini'], 'min_samples_split': [5], \n",
    "                 'max_depth': [i for i in range(20, 41)],\n",
    "                'min_samples_leaf': [5],\n",
    "                'n_estimators': [1000],\n",
    "                'class_weight': ['balanced', 'balanced_subsample']}]\n",
    "\n",
    "grid_search = GridSearchCV(Forest, param_search, cv=8, scoring='recall', n_jobs=3, verbose=2)\n",
    "grid_search.fit(Xtr_bool, Ytr)\n",
    "\n",
    "y_hat = grid_search.best_estimator_.predict(Xte_bool)\n",
    "\n",
    "print(confusion_matrix(Yte, y_hat))\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 84 candidates, totalling 672 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=3)]: Done 359 tasks      | elapsed: 39.1min\n",
      "[Parallel(n_jobs=3)]: Done 642 tasks      | elapsed: 73.1min\n",
      "[Parallel(n_jobs=3)]: Done 672 out of 672 | elapsed: 77.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4099  117]\n",
      " [  74  239]]\n",
      "RandomForestClassifier(class_weight='balanced', max_depth=40,\n",
      "                       min_samples_leaf=5, min_samples_split=5,\n",
      "                       n_estimators=1000)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest over TF-IDF Representation\n",
    "\n",
    "Forest = RandomForestClassifier()\n",
    "\n",
    "\n",
    "param_search = [{'criterion': ['entropy', 'gini'], 'min_samples_split': [5], \n",
    "                 'max_depth': [i for i in range(20, 41)],\n",
    "                'min_samples_leaf': [5],\n",
    "                'n_estimators': [1000],\n",
    "                'class_weight': ['balanced', 'balanced_subsample']}]\n",
    "\n",
    "grid_search = GridSearchCV(Forest, param_search, cv=8, scoring='recall', n_jobs=3, verbose=2)\n",
    "grid_search.fit(Xtr_tfidf, Ytr)\n",
    "\n",
    "y_hat = grid_search.best_estimator_.predict(Xte_tfidf)\n",
    "\n",
    "print(confusion_matrix(Yte, y_hat))\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4190   26]\n",
      " [  57  256]]\n",
      "SVC(C=256, kernel='linear')\n"
     ]
    }
   ],
   "source": [
    "# SVM; must be done over TF-IDF Representation\n",
    "\n",
    "SVM = svm.SVC()\n",
    "\n",
    "search_area = [\n",
    "  {'C': [2 ** i for i in list(range(-5,16))], 'kernel': ['linear']},\n",
    "  {'C': [2 ** i for i in list(range(-5,16))], 'gamma': [2 ** i for i in list(range(-15,4))], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "grid_search = GridSearchCV(SVM, search_area, cv=8, scoring='recall', n_jobs=3)\n",
    "grid_search.fit(Xtr_tfidf, Ytr)\n",
    "\n",
    "y_hat = grid_search.best_estimator_.predict(Xte_tfidf)\n",
    "\n",
    "print(confusion_matrix(Yte, y_hat))\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4180   36]\n",
      " [  93  220]]\n",
      "KNeighborsClassifier(n_neighbors=1, weights='distance')\n"
     ]
    }
   ],
   "source": [
    "# K Nearest Neighbors over TF-IDF Representaion with IPCA\n",
    "\n",
    "kNN = KNeighborsClassifier()\n",
    "param_search = [{'weights': ['distance'], 'n_neighbors': [i for i in range(1,51)]}]\n",
    "\n",
    "grid_search = GridSearchCV(kNN, param_search, cv=8, scoring='recall', n_jobs=3)\n",
    "grid_search.fit(Xtr_pca, Ytr)\n",
    "\n",
    "y_hat = grid_search.best_estimator_.predict(Xte_pca)\n",
    "\n",
    "print(confusion_matrix(Yte, y_hat))\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 42 candidates, totalling 336 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:  1.3min\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression over TF-IDF Representation with IPCA\n",
    "\n",
    "LReg = LogisticRegression()\n",
    "\n",
    "param_search = [{'penalty': ['l1', 'l2'],\n",
    "                'C': [2 ** i for i in range(-5, 16)],\n",
    "                'class_weight': ['balanced'],\n",
    "                'solver': ['liblinear']}]\n",
    "\n",
    "grid_search = GridSearchCV(LReg, param_search, cv=8, scoring='recall', n_jobs=3, verbose=2)\n",
    "grid_search.fit(Xtr_pca, Ytr)\n",
    "\n",
    "y_hat = grid_search.best_estimator_.predict(Xte_pca)\n",
    "\n",
    "print(confusion_matrix(Yte, y_hat))\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest over TF-IDF Representation with IPCA\n",
    "\n",
    "Forest = RandomForestClassifier()\n",
    "\n",
    "\n",
    "param_search = [{'criterion': ['entropy', 'gini'], 'min_samples_split': [5], \n",
    "                 'max_depth': [i for i in range(5, 31)],\n",
    "                'min_samples_leaf': [5],\n",
    "                'n_estimators': [1000],\n",
    "                'class_weight': ['balanced', 'balanced_subsample']}]\n",
    "\n",
    "grid_search = GridSearchCV(Forest, param_search, cv=8, scoring='recall', n_jobs=3, verbose=2)\n",
    "grid_search.fit(Xtr_pca, Ytr)\n",
    "\n",
    "y_hat = grid_search.best_estimator_.predict(Xte_pca)\n",
    "\n",
    "print(confusion_matrix(Yte, y_hat))\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM over TF-IDF Representation with IPCA\n",
    "\n",
    "SVM = svm.SVC()\n",
    "\n",
    "search_area = [\n",
    "  {'C': [2 ** i for i in list(range(-5,16))], 'kernel': ['linear']},\n",
    "  {'C': [2 ** i for i in list(range(-5,16))], 'gamma': [2 ** i for i in list(range(-15,4))], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "grid_search = GridSearchCV(SVM, search_area, cv=8, scoring='recall', verbose=2, n_jobs=3)\n",
    "grid_search.fit(Xtr_pca, Ytr)\n",
    "\n",
    "y_hat = grid_search.best_estimator_.predict(Xte_pca)\n",
    "\n",
    "print(confusion_matrix(Yte, y_hat))\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
